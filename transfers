I can see the issue! The problem is in how you're exporting and importing the CSV. When you export a single observation, it's creating a CSV with metadata lines at the top, but when you import it back, the code is treating those metadata lines as separate observations. Also, the CSV format might not be matching exactly what the import expects.

Let me fix this for you. Here are the key issues and solutions:

1. CSV Export Problem

Your export adds metadata lines at the top, but the import expects a pure CSV format starting with headers.

2. CSV Import/Format Mismatch

The import expects specific headers and formats that might not match your export.

Here's the updated JavaScript with fixes:

```javascript
// Export to CSV with proper format for re-import
exportToCSV(observations, dateFrom, dateTo, selectedIds) {
    const headers = [
        'ID', 'Timestamp', 'Observer Name', 'Scientific Name', 'Common Name', 'Location',
        'Latitude', 'Longitude', 'Habitat', 'Observation Type',
        'Characteristics', 'Equipment', 'References', 'Notes', 'Has Photo'
    ];
    
    // Create CSV content - NO METADATA LINES for re-import compatibility
    const rows = observations.map(obs => {
        // Format equipment as semicolon-separated string
        const equipmentStr = Array.isArray(obs.equipment) 
            ? obs.equipment.map(eq => {
                if (eq.startsWith('other: ')) {
                    return eq.substring(7);
                }
                return eq;
            }).join('; ') 
            : (obs.equipment || '');
        
        return [
            obs.id,
            obs.timestamp, // Keep as ISO string for import compatibility
            obs.observerName,
            obs.scientificName,
            obs.commonName || '',
            obs.location.text,
            obs.location.coordinates?.lat || '',
            obs.location.coordinates?.lng || '',
            obs.habitat,
            obs.observationType,
            obs.characteristics || '',
            equipmentStr,
            obs.references || '',
            obs.notes || '',
            obs.photo ? 'Yes' : 'No'
        ];
    });
    
    // Create CSV with proper escaping
    const csvContent = [
        headers.join(','), // Header row
        ...rows.map(row => 
            row.map(cell => {
                // Convert to string and escape if needed
                const str = String(cell);
                // If cell contains commas, quotes, or newlines, wrap in quotes
                if (str.includes(',') || str.includes('"') || str.includes('\n') || str.includes('\r')) {
                    return `"${str.replace(/"/g, '""')}"`; // Escape quotes
                }
                return str;
            }).join(',')
        )
    ].join('\n');
    
    const blob = new Blob([csvContent], { type: 'text/csv;charset=utf-8;' });
    const url = URL.createObjectURL(blob);
    
    const filename = selectedIds ? 
        `EcoLens_Selected_Export_${new Date().toISOString().slice(0,10)}.csv` :
        `EcoLens_Export_${new Date().toISOString().slice(0,10)}.csv`;
    
    this.showExportDownload(url, filename);
    this.showStatus(`Exported ${observations.length} observations to CSV`, 'success', 'export');
},

// Update the CSV template download to match export format
downloadCSVTemplate() {
    const headers = EXPECTED_CSV_HEADERS;
    
    // Example observation that matches the actual data structure
    const exampleRow = [
        'ECO-20231225-ABC12',
        '2023-12-25T14:30:00', // ISO format
        'Jane Doe',
        'Canis lupus',
        'Gray Wolf',
        'Yellowstone National Park, Wyoming',
        '44.4280',
        '-110.5885',
        'forest',
        'direct',
        'Large adult, gray coat, observed hunting',
        'binoculars; camera',
        'Field Guide to North American Mammals',
        'Observed from 100m distance',
        'No'
    ];
    
    // Create proper CSV with headers and one example row
    const csvContent = [
        headers.join(','),
        exampleRow.map(field => {
            // Escape fields that need it
            if (field.includes(',') || field.includes('"') || field.includes('\n')) {
                return `"${field.replace(/"/g, '""')}"`;
            }
            return field;
        }).join(',')
    ].join('\n');
    
    const blob = new Blob([csvContent], { type: 'text/csv;charset=utf-8;' });
    const url = URL.createObjectURL(blob);
    const link = document.createElement('a');
    link.href = url;
    link.download = 'EcoLens_Import_Template.csv';
    document.body.appendChild(link);
    link.click();
    document.body.removeChild(link);
    URL.revokeObjectURL(url);
    
    this.showStatus('CSV template downloaded', 'success', 'logbook');
},

// Improved CSV parsing to skip non-data rows
validateAndParseCSV(csvContent) {
    const result = {
        headers: [],
        rows: [],
        errors: [],
        warnings: [],
        validRows: 0,
        invalidRows: 0,
        warningRows: 0,
        totalRows: 0,
        summary: '',
        hasCoordinates: false,
        coordinates: []
    };

    try {
        // Split into lines and clean them
        const lines = csvContent.split('\n')
            .map(line => line.trim())
            .filter(line => line !== ''); // Remove empty lines
        
        if (lines.length < 2) {
            console.warn('CSV file is empty or has no data rows');
            throw new Error('CSV file is empty or has no data rows');
        }

        // Find the header row (look for expected headers)
        let headerIndex = -1;
        for (let i = 0; i < Math.min(10, lines.length); i++) { // Check first 10 lines for headers
            const potentialHeaders = this.parseCSVLine(lines[i]).map(h => h.trim().replace(/"/g, ''));
            // Check if this line contains any of our expected headers
            const hasExpectedHeaders = potentialHeaders.some(h => 
                EXPECTED_CSV_HEADERS.includes(h)
            );
            
            if (hasExpectedHeaders) {
                headerIndex = i;
                break;
            }
        }
        
        if (headerIndex === -1) {
            throw new Error('Could not find valid CSV headers. Make sure the file has headers like: ID, Timestamp, Observer Name, etc.');
        }

        // Parse headers
        result.headers = this.parseCSVLine(lines[headerIndex]).map(h => h.trim().replace(/"/g, ''));
        result.totalRows = lines.length - headerIndex - 1;

        // Check for expected headers
        const missingHeaders = EXPECTED_CSV_HEADERS.filter(h => !result.headers.includes(h));
        const extraHeaders = result.headers.filter(h => !EXPECTED_CSV_HEADERS.includes(h));

        if (missingHeaders.length > 0) {
            const warning = `Missing expected headers: ${missingHeaders.join(', ')}`;
            result.warnings.push(warning);
            console.warn(warning);
        }

        if (extraHeaders.length > 0) {
            const warning = `Extra headers found: ${extraHeaders.join(', ')}`;
            result.warnings.push(warning);
            console.warn(warning);
        }

        // Check if CSV has coordinate columns
        result.hasCoordinates = result.headers.includes('Latitude') && result.headers.includes('Longitude');

        // Parse data rows (skip header and any metadata lines before it)
        for (let i = headerIndex + 1; i < lines.length; i++) {
            const line = lines[i].trim();
            if (!line) continue;

            const rowErrors = [];
            const rowWarnings = [];
            
            try {
                const row = this.parseCSVLine(line);
                const rowData = {};
                
                // Map row data to headers
                result.headers.forEach((header, index) => {
                    rowData[header] = row[index] || '';
                });

                // Skip rows that look like metadata (e.g., "Export Metadata:")
                const firstCell = Object.values(rowData)[0] || '';
                if (typeof firstCell === 'string' && (
                    firstCell.includes('Export Metadata:') ||
                    firstCell.includes('Exported:') ||
                    firstCell.includes('Total Observations:') ||
                    firstCell.includes('Date Range:') ||
                    firstCell.includes('Selection:') ||
                    firstCell.includes('Exported by:') ||
                    firstCell.includes('App Version:')
                )) {
                    console.log('Skipping metadata row:', firstCell);
                    continue; // Skip this row
                }

                // Sanitize all values to prevent CSV injection
                Object.keys(rowData).forEach(key => {
                    if (typeof rowData[key] === 'string') {
                        rowData[key] = this.sanitizeCSVValue(rowData[key]);
                    }
                });

                // Validate required fields
                REQUIRED_CSV_HEADERS.forEach(header => {
                    if (result.headers.includes(header)) {
                        const value = rowData[header];
                        if (!value || value.trim() === '') {
                            const error = `Row ${i}: Missing required field "${header}"`;
                            rowErrors.push(error);
                            console.warn(error);
                        }
                    }
                });

                // Validate date format with clear guidance
                if (result.headers.includes('Timestamp') && rowData['Timestamp']) {
                    const date = this.parseDateWithWarning(rowData['Timestamp'], `row ${i}`);
                    if (date) {
                        rowData['_parsedTimestamp'] = date;
                    } else {
                        rowWarnings.push(`Row ${i}: Invalid date format. Use ISO format (YYYY-MM-DDTHH:mm:ss) like "2023-12-25T14:30:00"`);
                    }
                }

                // Validate and parse coordinates if present
                let hasValidCoords = false;
                if (result.hasCoordinates && rowData['Latitude'] && rowData['Longitude']) {
                    const lat = parseFloat(rowData['Latitude']);
                    const lng = parseFloat(rowData['Longitude']);
                    
                    if (isNaN(lat) || lat < -90 || lat > 90) {
                        const warning = `Row ${i}: Invalid Latitude value "${rowData['Latitude']}". Must be between -90 and 90.`;
                        rowWarnings.push(warning);
                        console.warn(warning);
                    } else if (isNaN(lng) || lng < -180 || lng > 180) {
                        const warning = `Row ${i}: Invalid Longitude value "${rowData['Longitude']}". Must be between -180 and 180.`;
                        rowWarnings.push(warning);
                        console.warn(warning);
                    } else {
                        hasValidCoords = true;
                        result.coordinates.push({ lat, lng });
                        
                        // Add coordinate validation message
                        rowData['_coordinateStatus'] = '✅ Valid coordinates';
                    }
                } else if (rowData['Latitude'] || rowData['Longitude']) {
                    const warning = `Row ${i}: Incomplete coordinates. Provide both Latitude and Longitude or leave both empty.`;
                    rowWarnings.push(warning);
                    console.warn(warning);
                }

                // Add to results
                result.rows.push({
                    data: rowData,
                    errors: rowErrors,
                    warnings: rowWarnings,
                    isValid: rowErrors.length === 0,
                    hasValidCoords: hasValidCoords,
                    rowNumber: i
                });

                if (rowErrors.length === 0) {
                    result.validRows++;
                    if (rowWarnings.length > 0) {
                        result.warningRows++;
                    }
                } else {
                    result.invalidRows++;
                }

                // Add errors and warnings to overall lists
                result.errors.push(...rowErrors);
                result.warnings.push(...rowWarnings);

            } catch (error) {
                const errorMsg = `Row ${i}: ${error.message}`;
                result.errors.push(errorMsg);
                console.error(errorMsg);
                result.rows.push({
                    data: null,
                    errors: [error.message],
                    warnings: [],
                    isValid: false,
                    hasValidCoords: false,
                    rowNumber: i
                });
                result.invalidRows++;
            }
        }

        // Update total rows based on actual parsed rows
        result.totalRows = result.rows.length;
        
        // Create summary
        result.summary = `Found ${result.totalRows} data rows: ${result.validRows} valid, ${result.invalidRows} invalid, ${result.warningRows} with warnings`;

        return result;

    } catch (error) {
        const errorMsg = `CSV parsing error: ${error.message}`;
        result.errors.push(errorMsg);
        console.error(errorMsg);
        return result;
    }
},

// Also update the importObservations function to better handle the data
importObservations(validationResult, mode = 'append') {
    try {
        const importedObservations = [];
        let successCount = 0;
        let errorCount = 0;
        let updateCount = 0;

        // Process only valid rows
        validationResult.rows.forEach(row => {
            if (row.isValid && row.data) {
                try {
                    // Parse equipment string into array
                    let equipmentArray = [];
                    if (row.data['Equipment']) {
                        equipmentArray = row.data['Equipment'].split(';')
                            .map(item => item.trim())
                            .filter(item => item !== '');
                    }
                    
                    // Create observation with proper data mapping
                    const observation = this.normalizeObservation({
                        id: row.data['ID'] || this.generateObservationId(),
                        timestamp: row.data['_parsedTimestamp'] || row.data['Timestamp'] || new Date().toISOString(),
                        observerName: row.data['Observer Name'] || '',
                        scientificName: row.data['Scientific Name'] || '',
                        commonName: row.data['Common Name'] || '',
                        location: {
                            text: row.data['Location'] || '',
                            coordinates: (row.data['Latitude'] && row.data['Longitude']) ? {
                                lat: parseFloat(row.data['Latitude']),
                                lng: parseFloat(row.data['Longitude'])
                            } : null
                        },
                        habitat: row.data['Habitat'] || '',
                        observationType: row.data['Observation Type'] || 'direct',
                        characteristics: row.data['Characteristics'] || '',
                        equipment: equipmentArray,
                        references: row.data['References'] || '',
                        notes: row.data['Notes'] || '',
                        source: 'import'
                    });

                    importedObservations.push(observation);
                    successCount++;
                } catch (error) {
                    console.warn('Error creating observation from row:', error, row.data);
                    errorCount++;
                }
            } else {
                errorCount++;
            }
        });

        if (importedObservations.length === 0) {
            throw new Error('No valid observations found in CSV file');
        }

        // Handle import based on mode
        const existingIds = new Set(this.observations.map(obs => obs.id));
        
        switch (mode) {
            case 'replace':
                // Completely replace existing data
                this.observations = importedObservations;
                break;
                
            case 'merge':
                // Merge and update existing observations
                const observationMap = new Map();
                
                // Add all existing observations to map
                this.observations.forEach(obs => {
                    observationMap.set(obs.id, obs);
                });
                
                // Merge imported observations
                importedObservations.forEach(importedObs => {
                    if (observationMap.has(importedObs.id)) {
                        // Update existing observation
                        const existingObs = observationMap.get(importedObs.id);
                        observationMap.set(importedObs.id, this.normalizeObservation({
                            ...existingObs,
                            ...importedObs,
                            metadata: {
                                ...existingObs.metadata,
                                source: 'merged'
                            }
                        }));
                        updateCount++;
                    } else {
                        // Add new observation
                        observationMap.set(importedObs.id, importedObs);
                    }
                });
                
                this.observations = Array.from(observationMap.values());
                break;
                
            case 'append':
            default:
                // Append, avoiding duplicates by ID
                const uniqueImports = importedObservations.filter(obs => !existingIds.has(obs.id));
                this.observations = [...this.observations, ...uniqueImports];
                break;
        }

        // Sort by timestamp (newest first)
        this.observations.sort((a, b) => new Date(b.timestamp) - new Date(a.timestamp));
        
        // Save to localStorage
        localStorage.setItem('ecolens_observations', JSON.stringify(this.observations));
        this.renderLogbook();

        return {
            success: true,
            imported: successCount,
            updated: updateCount,
            errors: errorCount,
            total: this.observations.length,
            mode: mode
        };

    } catch (error) {
        console.error('Import error:', error);
        throw error;
    }
},
```

Key Changes Made:

1. Fixed CSV Export: Removed metadata lines from the CSV export so it's pure CSV format that can be re-imported.
2. Improved CSV Import: Added logic to:
   · Find the header row dynamically (in case there are metadata lines)
   · Skip rows that look like metadata
   · Better parse equipment strings into arrays
   · Handle missing or empty values gracefully
3. Updated Template Download: Made the template match the actual export format.

To Test:

1. Export a single observation as CSV
2. Clear all observations
3. Import the CSV you just exported
4. Check if it imports correctly as one observation

The import should now:

· Skip any metadata lines
· Parse the single observation correctly
· Preserve all the data fields

If you're still having issues, try this: After exporting a CSV, open it in a text editor and check what the actual content looks like. The first line should be headers, and the second line should be your data. If there are extra lines at the top (metadata), delete them and save the file, then try importing again.
